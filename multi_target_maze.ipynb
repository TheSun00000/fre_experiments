{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import collections\n",
    "\n",
    "from dm_control import mujoco, viewer, suite\n",
    "from dm_control.rl import control\n",
    "from dm_control.suite import base, common\n",
    "from dm_control.suite.utils import randomizers\n",
    "from dm_control.utils import rewards\n",
    "from dm_control.utils import io as resources\n",
    "from dm_env import specs\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [('reach_top_left', np.array([-0.15, 0.15, 0.01])),\n",
    "         ('reach_top_right', np.array([0.15, 0.15, 0.01])),\n",
    "         ('reach_bottom_left', np.array([-0.15, -0.15, 0.01])),\n",
    "         ('reach_bottom_right', np.array([0.15, -0.15, 0.01]))]\n",
    "\n",
    "\n",
    "class MultiTaskPointMassMaze(base.Task):\n",
    "    \"\"\"A point_mass `Task` to reach target with smooth reward.\"\"\"\n",
    "    def __init__(self, target_id, random=None):\n",
    "        \"\"\"Initialize an instance of `PointMassMaze`.\n",
    "\n",
    "    Args:\n",
    "      randomize_gains: A `bool`, whether to randomize the actuator gains.\n",
    "      random: Optional, either a `numpy.random.RandomState` instance, an\n",
    "        integer seed for creating a new `RandomState`, or None to select a seed\n",
    "        automatically (default).\n",
    "    \"\"\"\n",
    "        self._target = TASKS[target_id][1]\n",
    "        super().__init__(random=random)\n",
    "\n",
    "    def initialize_episode(self, physics):\n",
    "        \"\"\"Sets the state of the environment at the start of each episode.\n",
    "\n",
    "       If _randomize_gains is True, the relationship between the controls and\n",
    "       the joints is randomized, so that each control actuates a random linear\n",
    "       combination of joints.\n",
    "\n",
    "    Args:\n",
    "      physics: An instance of `mujoco.Physics`.\n",
    "    \"\"\"\n",
    "        randomizers.randomize_limited_and_rotational_joints(\n",
    "            physics, self.random)\n",
    "        physics.data.qpos[0] = np.random.uniform(-0.29, -0.15)\n",
    "        physics.data.qpos[1] = np.random.uniform(0.15, 0.29)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        physics.named.data.geom_xpos['target'][:] = self._target\n",
    "        \n",
    "\n",
    "        super().initialize_episode(physics)\n",
    "\n",
    "    def get_observation(self, physics):\n",
    "        \"\"\"Returns an observation of the state.\"\"\"\n",
    "        obs = collections.OrderedDict()\n",
    "        obs['position'] = physics.position()\n",
    "        obs['velocity'] = physics.velocity()\n",
    "        return obs\n",
    "    \n",
    "    def get_reward_spec(self):\n",
    "        return specs.Array(shape=(1,), dtype=np.float32, name='reward')\n",
    "\n",
    "    def get_reward(self, physics):\n",
    "        \"\"\"Returns a reward to the agent.\"\"\"\n",
    "        target_size = .015\n",
    "        control_reward = rewards.tolerance(physics.control(), margin=1,\n",
    "                                       value_at_margin=0,\n",
    "                                       sigmoid='quadratic').mean()\n",
    "        small_control = (control_reward + 4) / 5\n",
    "        near_target = rewards.tolerance(physics.mass_to_target_dist(self._target),\n",
    "                                bounds=(0, target_size), margin=target_size)\n",
    "        reward = near_target * small_control\n",
    "        return reward\n",
    "\n",
    "\n",
    "\n",
    "class Physics(mujoco.Physics):\n",
    "    \"\"\"physics for the point_mass domain.\"\"\"\n",
    "\n",
    "    def mass_to_target_dist(self, target):\n",
    "        \"\"\"Returns the distance from mass to the target.\"\"\"\n",
    "        d = target - self.named.data.geom_xpos['pointmass']\n",
    "        return np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_dm_control_to_gym_space(dm_control_space):\n",
    "    r\"\"\"Convert dm_control space to gym space. \"\"\"\n",
    "    if isinstance(dm_control_space, specs.BoundedArray):\n",
    "        space = spaces.Box(low=dm_control_space.minimum, \n",
    "                           high=dm_control_space.maximum, \n",
    "                           dtype=dm_control_space.dtype)\n",
    "        assert space.shape == dm_control_space.shape\n",
    "        return space\n",
    "    elif isinstance(dm_control_space, specs.Array) and not isinstance(dm_control_space, specs.BoundedArray):\n",
    "        space = spaces.Box(low=-float('inf'), \n",
    "                           high=float('inf'), \n",
    "                           shape=dm_control_space.shape, \n",
    "                           dtype=dm_control_space.dtype)\n",
    "        return space\n",
    "    elif isinstance(dm_control_space, dict):\n",
    "        space = spaces.Dict({key: convert_dm_control_to_gym_space(value)\n",
    "                             for key, value in dm_control_space.items()})\n",
    "        return space\n",
    "\n",
    "\n",
    "class DMSuiteEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array'],\n",
    "                         'video.frames_per_second': round(1.0/self.env.control_timestep())}\n",
    "\n",
    "        self.observation_space = convert_dm_control_to_gym_space(self.env.observation_spec())\n",
    "        self.action_space = convert_dm_control_to_gym_space(self.env.action_spec())\n",
    "        self.viewer = None\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        return self.env.task.random.seed(seed)\n",
    "    \n",
    "    def step(self, action):\n",
    "        timestep = self.env.step(action)\n",
    "        observation = timestep.observation\n",
    "        reward = timestep.reward\n",
    "        done = timestep.last()\n",
    "        info = {}\n",
    "        truncated = False\n",
    "        return observation, reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        timestep = self.env.reset()\n",
    "        return timestep.observation, {}\n",
    "    \n",
    "    def render(self, mode='human', **kwargs):\n",
    "        if 'camera_id' not in kwargs:\n",
    "            kwargs['camera_id'] = 0  # Tracking camera\n",
    "        use_opencv_renderer = kwargs.pop('use_opencv_renderer', False)\n",
    "        \n",
    "        img = self.env.physics.render(**kwargs)\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            if self.viewer is None:\n",
    "                if not use_opencv_renderer:\n",
    "                    from gym.envs.classic_control import rendering\n",
    "                    self.viewer = rendering.SimpleImageViewer(maxwidth=1024)\n",
    "                else:\n",
    "                    from . import OpenCVImageViewer\n",
    "                    self.viewer = OpenCVImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "        return self.env.close()\n",
    "    \n",
    "    \n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # Flatten the observation space by combining the shapes of each dictionary entry\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf, \n",
    "            shape=(self.flatten_observation_space_shape(),), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def flatten_observation_space_shape(self):\n",
    "        # Calculate the total number of elements in the observation space after flattening\n",
    "        total_shape = 0\n",
    "        for key in self.observation_space.spaces:\n",
    "            total_shape += np.prod(self.observation_space.spaces[key].shape)\n",
    "        return total_shape\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Flatten the dictionary of numpy arrays into a single vector\n",
    "        return np.concatenate([obs[key].flatten() for key in obs], axis=0)\n",
    "    \n",
    "def flat_obs(obs):\n",
    "    return np.concatenate([obs[key].flatten() for key in obs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "target_id = 3\n",
    "\n",
    "xml = resources.GetResource(f'mazes/point_mass_maze_{TASKS[target_id][0]}.xml')\n",
    "physics = Physics.from_xml_string(xml, common.ASSETS)\n",
    "task = MultiTaskPointMassMaze(target_id=target_id)\n",
    "\n",
    "dm_env = control.Environment(\n",
    "    physics,\n",
    "    task,\n",
    "    time_limit=20,\n",
    ")\n",
    "\n",
    "def get_reward(self, physics):\n",
    "    \"\"\"Returns a reward to the agent.\"\"\"\n",
    "    # print(physics.named.data.geom_xpos['pointmass'])\n",
    "    target_size = physics.named.data.geom_xpos['target', 0]\n",
    "    distance = np.linalg.norm(physics.named.data.geom_xpos['target'] - physics.named.data.geom_xpos['pointmass'])\n",
    "    distance2 = rewards.tolerance(distance, bounds=(0, target_size), margin=target_size)\n",
    "    # print(distance, distance2)\n",
    "    reward = 1 - 2*distance\n",
    "    return reward\n",
    "\n",
    "dm_env.task.get_reward = MethodType(get_reward, dm_env.task.get_reward)\n",
    "\n",
    "target_id = 0\n",
    "xml = resources.GetResource(f'mazes/point_mass_maze_{TASKS[target_id][0]}.xml')\n",
    "dm_env.physics.reload_from_xml_string(xml, common.ASSETS)\n",
    "\n",
    "viewer.launch(dm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.launch(dm_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geom_xmat', 'geom_xpos']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in dm_env.physics.named.data.__dir__() if 'geom' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9223131121790506e-48"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_size = 0.015\n",
    "distance = np.linalg.norm(dm_env.physics.named.data.geom_xpos['target'] - dm_env.physics.named.data.geom_xpos['pointmass'])\n",
    "goal_reached = rewards.tolerance(distance, bounds=(0, target_size), margin=target_size)\n",
    "if goal_reached > 0.9:\n",
    "    print('done')\n",
    "goal_reached\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
